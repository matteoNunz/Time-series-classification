{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMPRQ1t9gVDZ"
      },
      "outputs": [],
      "source": [
        "def add_average_values(X, scale=1):\n",
        "  \"\"\"\n",
        "  This method extend the original dataset adding between the original values\n",
        "    new data that are a random weighted average of the previous and the next timestamps.\n",
        "  :param X: the original dataset to extend\n",
        "  :param scale: define how many timestamps add between 2 original timestamps\n",
        "  :return: the extended dataset \n",
        "  \"\"\"\n",
        "  new_X = np.zeros((X.shape[0], X.shape[1] + scale * X.shape[1] - scale, X.shape[2]))\n",
        "\n",
        "  for sample in tqdm(range(X.shape[0])):\n",
        "    new_sample = []\n",
        "    for i in range(X[sample].shape[0]):\n",
        "      # Append the original value\n",
        "      new_sample.append(X[sample,i,:])\n",
        "      for _ in range(scale):\n",
        "        # If there is an other element\n",
        "        if i != X[sample].shape[0] - 1:\n",
        "          # Append the (random) average with the next one\n",
        "          alpha = np.random.random()\n",
        "          new_sample.append(alpha * X[sample,i,:] + (1-alpha) * X[sample,i+1,:])\n",
        "    # Append the new sample\n",
        "    new_X[sample] = np.array(new_sample)\n",
        "\n",
        "  return new_X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def SMOTE(x, num_of_samples=100, num_to_combine=2):\n",
        "  \"\"\"\n",
        "  Implementation of SMOTE for oversampling\n",
        "  :input x: is the class samples to oversample\n",
        "  :input num_of_samples: is the num of samples to reach for the specific class\n",
        "  :input num_to_combine: is the number of sample to linearly combine to get the new one\n",
        "  :return: a list containing all the elements of x plus the new samples\n",
        "  \"\"\"\n",
        "  # This is a possible addition to do downsampling\n",
        "  # If a class has already a lot of samples, remove some of them and keep just the first ones\n",
        "  #stop = min(x.shape[0]-1, num_of_samples-1)\n",
        "\n",
        "  #if x.shape[0] > num_of_samples:\n",
        "  #  stop = num_of_samples // 3\n",
        "  #  return list(x[:stop])\n",
        "  #else:\n",
        "  stop = x.shape[0] - 1\n",
        "\n",
        "  # Get all the original index\n",
        "  original_index = np.arange(start=0, stop=stop, step=1)\n",
        "\n",
        "  # Create a list with all the elements\n",
        "  X_list = list(x)\n",
        "\n",
        "  # Till when not enough samples have been created\n",
        "  while(len(X_list) < num_of_samples):\n",
        "    # Select num_to_combine sequences\n",
        "    selected_index = np.random.choice(original_index, size=num_to_combine, replace=False)\n",
        "\n",
        "    # toDo: extend the method to allow combination of more than 2 sequences\n",
        "\n",
        "    # Get the random position between the 2 vectors\n",
        "    probability_position = np.random.random() \n",
        "\n",
        "    # Create a new array with the same size of the sequences\n",
        "    new_sample = np.zeros((x[0].shape))\n",
        "\n",
        "    # Combine the selected sequences \n",
        "    for row in range(new_sample.shape[0]):\n",
        "      for feature in range(new_sample.shape[1]):\n",
        "        new_sample[row, feature] = probability_position * x[selected_index[0], row, feature] + \\\n",
        "                                (1 - probability_position) * x[selected_index[1], row, feature]\n",
        "\n",
        "    # Append the new sequence\n",
        "    X_list.append(new_sample)\n",
        "\n",
        "  return X_list"
      ],
      "metadata": {
        "id": "gt-rzWaghJJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomSequenceAugmentation(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  This class extend the tf.keras.layers.Layer\n",
        "  The idea of this class is to create the network with the augmentation built in.\n",
        "  \"\"\"\n",
        "  def __init__(self, sigma_s=0.1, sigma_j=1, verbose=False, prob=0.5, **kwargs):\n",
        "    \"\"\"\n",
        "    Initiliaze the layer\n",
        "    :param sigma_s: it is the scaling defining the normal distribution to sample\n",
        "    :param sigma_j: it is the scaling defining the normal distribution to sample\n",
        "    :param verbose: if True, print some operations done\n",
        "    :param prob: it defines the probability of applying an augmentation\n",
        "    \"\"\"\n",
        "    super(RandomSequenceAugmentation, self).__init__(**kwargs)\n",
        "    self.sigma_s = sigma_s\n",
        "    self.sigma_j = sigma_j\n",
        "    self.verbose = verbose\n",
        "    self.verbose = verbose\n",
        "    self.prob = prob\n",
        "\n",
        "  def call(self, sequence,  training=None):\n",
        "    \"\"\"\n",
        "    This method is called and it's effective only during the training phase.\n",
        "    It applies 3 different augmentation techniques: scaling, jittering and flipping.\n",
        "    :param sequence: it's the current input sequence in the network\n",
        "    :param training: it says whether the phase is training or test\n",
        "    :return: the augmented sequence\n",
        "    \"\"\"\n",
        "    # If it's not training, do not apply anything\n",
        "    if not training:\n",
        "        return sequence\n",
        "\n",
        "    if self.verbose:\n",
        "      print(\"Type of sequence is {} and its shape is {}\".format(type(sequence), sequence.shape))\n",
        "\n",
        "    aug_sequence = sequence\n",
        "\n",
        "    # Apply scaling augmentation with some probability\n",
        "    if np.random.random() >= self.prob:\n",
        "      if self.verbose:\n",
        "        print(\"Applying scaling augmentation\")\n",
        "      factor = np.random.normal(loc=1., scale=self.sigma_s, size=(aug_sequence.shape[1], aug_sequence.shape[2]))\n",
        "      aug_sequence = tf.multiply(aug_sequence, factor)\n",
        "\n",
        "    # Apply jitter augmentation with some probability\n",
        "    if np.random.random() >= self.prob:\n",
        "      if self.verbose:\n",
        "        print(\"Applying jitter augmentation\")\n",
        "      aug_sequence = aug_sequence + np.random.normal(loc=0., scale=self.sigma_j, size=(aug_sequence.shape[1], aug_sequence.shape[2]))\n",
        "\n",
        "    # Apply flip augmentation with some probability\n",
        "    if np.random.random() >= self.prob:\n",
        "      if self.verbose:\n",
        "        print(\"Applying flip augmentation\")\n",
        "      aug_sequence = aug_sequence * (-1.0)\n",
        "\n",
        "    return aug_sequence "
      ],
      "metadata": {
        "id": "0ryz9bLkslr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_selection(X_train, X_val, X_test, Y_train, Y_val, Y_test):\n",
        "  \"\"\"\n",
        "  This method, given a dataset, tries to find an optimal subset of the original\n",
        "    features, selecting just the most significant one.\n",
        "  :param X_train: original training datased with all the features\n",
        "  :param X_val: original validation datased with all the features\n",
        "  :param X_test: original test datased with all the features\n",
        "  :param Y_train: the labels of each sample in the training set\n",
        "  :param Y_val: the labels of each sample in the validation set\n",
        "  :param Y_test: the labels of each sample in the test set\n",
        "  :return: the subset of features selected and the corresponding accuracy.\n",
        "           Optionally, it's possible to modify the the method in order to return\n",
        "           also the optimal model already trained.\n",
        "  \"\"\"\n",
        "\n",
        "  # Start with no feature selected\n",
        "  old_selected_features = []\n",
        "  selected_features = []\n",
        "  optimal_features = []\n",
        "\n",
        "  # All the available feature\n",
        "  features = [i for i in range(X_train.shape[2])]\n",
        "  old_accuracy = -1.0\n",
        "  accuracy = 0.0\n",
        "\n",
        "  accuracies = []\n",
        "  analysed_models = []\n",
        "\n",
        "  # Go on till there is an improvement in the accuracy or till when all the features have been added\n",
        "  while accuracy > old_accuracy and len(optimal_features) <= len(features):\n",
        "    print(\"Current optimal features are {} with accuracy of {}\".format(optimal_features, round(accuracy, 4)))\n",
        "\n",
        "    # Get the features that are still available \n",
        "    available_feature = set(features) - set(optimal_features)\n",
        "\n",
        "    # Save the data of the last iteration\n",
        "    old_features = optimal_features.copy()\n",
        "    old_accuracy = accuracy\n",
        "\n",
        "    # Forward step\n",
        "    for feature in available_feature:\n",
        "      # Append the new feature\n",
        "      selected_features = optimal_features.copy()\n",
        "      selected_features.append(feature)\n",
        "      # Order the list\n",
        "      selected_features.sort()\n",
        "\n",
        "      # If the configuration has already been evaluated\n",
        "      if selected_features in analysed_models:\n",
        "        continue\n",
        "\n",
        "      # Create the new dataset arrays (with the new number of features)\n",
        "      X_train_f = np.zeros((X_train.shape[0], X_train.shape[1], len(selected_features)))\n",
        "      X_val_f = np.zeros((X_val.shape[0], X_val.shape[1], len(selected_features)))\n",
        "      X_test_f = np.zeros((X_test.shape[0], X_test.shape[1], len(selected_features)))\n",
        "\n",
        "      # Save the new features only\n",
        "      X_train_f = X_train[:,:,selected_features]\n",
        "      X_val_f = X_val[:,:,selected_features]\n",
        "      X_test_f = X_test[:,:,selected_features]\n",
        "\n",
        "      # Compute the predictions\n",
        "      # Compute prediction is a method that instanciate the wanted NN structure, compile and fit the model.\n",
        "      #   Then it already computes the prediction on the test set, returning the predictions.\n",
        "      predictions = compute_prediction(X_train_f=X_train_f, \n",
        "                                      Y_train=Y_train, \n",
        "                                      X_val_f=X_val_f,\n",
        "                                      Y_val=Y_val,\n",
        "                                      X_test_f=X_test_f\n",
        "                                      )\n",
        "\n",
        "      # Compute the accuracy\n",
        "      accuracy_prediction = accuracy_score(np.argmax(Y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "      print('Accuracy with features {} is {}'.format(selected_features, round(accuracy_prediction, 4)))\n",
        "\n",
        "      # Save the results of this evaluation\n",
        "      accuracies.append(accuracy_prediction)\n",
        "      analysed_models.append(selected_features)\n",
        "\n",
        "    # Now select the best model evaluated in this iteraion\n",
        "    best_model_index = np.argmax(np.array(accuracies))\n",
        "\n",
        "    # If there is an improvement in the accuracy\n",
        "    if accuracies[best_model_index] >= old_accuracy:\n",
        "      optimal_features = analysed_models[best_model_index].copy()\n",
        "      accuracy = accuracies[best_model_index]\n",
        "\n",
        "    # Backpropagation step\n",
        "    # Try to remove one of the old features and see if the perfromance incerase\n",
        "    # For all the features thet were present before\n",
        "    for feature in old_features:\n",
        "      # Start from the last optimal configuration\n",
        "      selected_features = analysed_models[best_model_index].copy()\n",
        "      # Remove one old feature\n",
        "      selected_features.remove(feature)\n",
        "\n",
        "      # If the configuration has already been evaluated\n",
        "      if selected_features in analysed_models:\n",
        "        continue\n",
        "\n",
        "      # Create the new dataset arrays (with the new number of features)\n",
        "      X_train_f = np.zeros((X_train.shape[0], X_train.shape[1], len(selected_features)))\n",
        "      X_val_f = np.zeros((X_val.shape[0], X_val.shape[1], len(selected_features)))\n",
        "      X_test_f = np.zeros((X_test.shape[0], X_test.shape[1], len(selected_features)))\n",
        "\n",
        "      # Save the new features only\n",
        "      X_train_f = X_train[:,:,selected_features]\n",
        "      X_val_f = X_val[:,:,selected_features]\n",
        "      X_test_f = X_test[:,:,selected_features]\n",
        "\n",
        "      # Compute the predictions\n",
        "      # Compute prediction is a method that instanciate the wanted NN structure, compile and fit the model.\n",
        "      #   Then it already computes the prediction on the test set, returning the predictions.\n",
        "      predictions = compute_prediction(X_train_f=X_train_f, \n",
        "                                      Y_train=Y_train, \n",
        "                                      X_val_f=X_val_f,\n",
        "                                      Y_val=Y_val,\n",
        "                                      X_test_f=X_test_f, \n",
        "                                      class_weights=class_weight\n",
        "                                      )\n",
        "      \n",
        "      # Compute the accuracy\n",
        "      accuracy_prediction = accuracy_score(np.argmax(Y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
        "      print('Accuracy with features {} is {}'.format(selected_features, round(accuracy_prediction, 4)))\n",
        "\n",
        "      # Save the results of this evaluation\n",
        "      accuracies.append(accuracy_prediction)\n",
        "      analysed_models.append(selected_features)\n",
        "\n",
        "    # Now select the best model evaluated in this iteraion\n",
        "    best_model_index = np.argmax(np.array(accuracies))\n",
        "\n",
        "    # If there is an improvement in the accuracy (max between the old one and the forward one)\n",
        "    if accuracies[best_model_index] > max(old_accuracy, accuracy):\n",
        "      optimal_features = analysed_models[best_model_index]\n",
        "      accuracy = accuracies[best_model_index]\n",
        "\n",
        "  print('Optimal features are {} with an accuracy of {}'.format(optimal_features, round(accuracy,4)))\n",
        "  # toDo: this method is easely extendable in order to return directly the best model trained\n",
        "  return optimal_features, accuracy"
      ],
      "metadata": {
        "id": "WUXg-7CbhiuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_prediction(X_train_f, Y_train, X_val_f, Y_val, X_test_f, class_weights=None):\n",
        "  \"\"\"\n",
        "  This method instanciate the wanted NN structure, compile and fit the model.\n",
        "    Then it already computes the prediction on the test set, returning the predictions.\n",
        "  :param X_train_f: training datased with only a subset of features\n",
        "  :param Y_train: the labels of each sample in the training set\n",
        "  :param X_val_f: validation datased with only a subset of features\n",
        "  :param Y_val: the labels of each sample in the validation set\n",
        "  :param X_test_f: test datased with only a subset of features\n",
        "  :param class_weight: optional, weights to use during the training\n",
        "  :return: the predictions of the model trained\n",
        "  \"\"\"\n",
        "  # Define the training paramenters\n",
        "  input_shape = X_train_f.shape[1:]\n",
        "  classes = Y_train.shape[-1]\n",
        "  batch_size = 64\n",
        "  epochs = 200\n",
        "\n",
        "  # Build the model\n",
        "  model = build_network(input_shape, classes)\n",
        "\n",
        "  # Fit the model\n",
        "  model.fit(x = X_train_f,\n",
        "            y = Y_train,\n",
        "            batch_size = batch_size,\n",
        "            epochs = epochs,\n",
        "            #class_weight=class_weight,\n",
        "            verbose=0,\n",
        "            validation_data=(X_val_f, Y_val),\n",
        "            callbacks = [\n",
        "                tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=30, restore_best_weights=True),\n",
        "                tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=10, factor=0.5, min_lr=1e-5)\n",
        "            ])\n",
        "\n",
        "  # Compute the prediction on the test set\n",
        "  predictions = model.predict(X_test_f)\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "kepnQlzIio2z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}